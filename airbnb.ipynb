{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du dataframe a partir du fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"airbnb_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des premieres lignes\n",
    "\n",
    "Ici on remarque que les données ne sont pas en float ou int donc on ne pourra pas faire d'operation, il faut donc que les convertir en valeurs numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage de statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netoyage de donnée\n",
    "\n",
    "## Classe `Netoyage`\n",
    "Cette classe permet de transformer le DataFrame en lui appliquant plusieurs methodes.\n",
    "\n",
    "### Méthode `__init__`\n",
    "- **Paramètre** :\n",
    "  - `df` : Le DataFrame à transformer.\n",
    "- **Action** : Initialise la classe avec le DataFrame.\n",
    "\n",
    "### Méthode `transform`\n",
    "- **Action** : Applique les différentes méthode de transformations au DataFrame.\n",
    "- **Retour** : Retourne le DataFrame transformé.\n",
    "\n",
    "### Méthode `convert_text`\n",
    "- **Action** : Convertit les colonnes textuelles en valeurs numériques en remplaçant les valeurs uniques par des indices.\n",
    "- **Colonnes concernées** : `neighbourhood`, `property_type`, `city`, `room_type`, `bed_type`, `cancellation_policy`.\n",
    "\n",
    "### Méthode `convert_rate`\n",
    "- **Action** : Convertit la colonne `host_response_rate` en pourcentage numérique et remplace les valeurs manquantes par 0.\n",
    "- **Colonne concernée** : `host_response_rate`.\n",
    "\n",
    "### Méthode `convert_bool`\n",
    "- **Action** : Convertit les colonnes booléennes textuelles en valeurs numériques (1 pour `t` ou `True`, 0 pour autres).\n",
    "- **Colonnes concernées** : `host_has_profile_pic`, `host_identity_verified`, `instant_bookable`, `cleaning_fee`.\n",
    "\n",
    "### Méthode `convert_date`\n",
    "- **Action** : Convertit les colonnes de date en nombre de jours écoulés depuis la date actuelle et remplace les valeurs manquantes par 0.\n",
    "- **Colonnes concernées** : `first_review`, `last_review`, `host_since`.\n",
    "\n",
    "### Méthode `replace_caracter`\n",
    "- **Action** : Remplace les caractères spéciaux dans les noms de colonnes par des underscores.\n",
    "- **Caractères remplacés** : `[^/{(&)}:-]+` et les espaces.\n",
    "\n",
    "### Méthode `drop_unnecessary_columns`\n",
    "- **Action** : Supprime les colonnes non utiles du DataFrame.\n",
    "- **Colonnes supprimées** : `id`, `description`, `name`, `zipcode`.\n",
    "\n",
    "### Méthode `handle_missing_values`\n",
    "- **Action** : Remplit les valeurs manquantes par 0 dans le DataFrame.\n",
    "\n",
    "### Méthode `create_amenities_columns`\n",
    "- **Action** : Crée des colonnes binaires pour chaque commodité unique présente dans la colonne `amenities`.\n",
    "- **Détail** :\n",
    "  - Transforme la colonne `amenities` en une liste de commodités.\n",
    "  - Crée une colonne pour chaque commodité unique avec des valeurs 1 ou 0 selon la présence de la commodité.\n",
    "  - Supprime la colonne originale `amenities`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class Netoyage:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def transform(self):\n",
    "        self.create_amenities_columns()\n",
    "        self.drop_unnecessary_columns()\n",
    "        self.convert_text()\n",
    "        self.convert_bool()\n",
    "        self.convert_date()\n",
    "        self.convert_rate()\n",
    "        self.handle_missing_values()\n",
    "        self.replace_caracter()\n",
    "        return self.df\n",
    "    \n",
    "    def convert_text(self):\n",
    "        tab = ['neighbourhood','property_type','city','room_type','bed_type','cancellation_policy']\n",
    "        for col in tab:\n",
    "            properties = self.df[col].unique()\n",
    "            property2index = {prop:i for (i, prop) in enumerate(properties)}\n",
    "            max_index = max(list(property2index.values()))\n",
    "            self.df.loc[:, col] = self.df[col].replace(property2index)\n",
    "            self.df.loc[self.df[col].map(type).eq(str),col] = np.nan\n",
    "            self.df[col].fillna(max_index + 1)\n",
    "\n",
    "\n",
    "    def convert_rate(self):\n",
    "        self.df['host_response_rate']= self.df['host_response_rate'].str.rstrip('%').astype(float)\n",
    "        self.df['host_response_rate']= self.df['host_response_rate'].fillna(0)\n",
    "        \n",
    "    def convert_bool(self):\n",
    "        tab = ['host_has_profile_pic','host_identity_verified','instant_bookable','cleaning_fee']\n",
    "        for col in tab:\n",
    "            self.df[col] = self.df[col].apply(lambda x: 1 if (x == 't' or x == 'True') else 0)\n",
    "            self.df[col] = self.df[col].fillna(2)\n",
    "\n",
    "\n",
    "    def convert_date(self):\n",
    "        tab = ['first_review', 'last_review', 'host_since']\n",
    "        for col in tab:\n",
    "            self.df[col] = pd.to_datetime(self.df[col])\n",
    "            today = datetime.now()\n",
    "            self.df[col] = (today - self.df[col]).dt.days\n",
    "            self.df[col] = self.df[col].fillna(0)\n",
    "\n",
    "    def replace_caracter(self):\n",
    "        tab = '[^/{(&)}:-]+'\n",
    "        for i in tab:\n",
    "            self.df.columns = self.df.columns.str.replace(i, '_',)\n",
    "        self.df.columns = self.df.columns.str.replace(' ', '_',)\n",
    "\n",
    "    def drop_unnecessary_columns(self):\n",
    "        columns_to_drop = ['id', 'description','name','zipcode']\n",
    "        self.df = self.df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        self.df = self.df.fillna(0)\n",
    "    \n",
    "    def create_amenities_columns(self):\n",
    "        self.df['amenities'] = self.df['amenities'].apply(lambda x: x.strip('{}').replace('\"', '').split(','))\n",
    "        unique_amenities = set(amenity.strip() for amenities_list in self.df['amenities'] for amenity in amenities_list)\n",
    "        amenities_dict = {f'amenity_{amenity}': [] for amenity in unique_amenities}\n",
    "        for amenities_list in self.df['amenities']:\n",
    "            for amenity in unique_amenities:\n",
    "                amenities_dict[f'amenity_{amenity}'].append(1 if amenity in amenities_list else 0)\n",
    "        amenities_df = pd.DataFrame(amenities_dict)\n",
    "        self.df = pd.concat([self.df, amenities_df], axis=1)\n",
    "        self.df = self.df.drop(columns=['amenities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation d'un objet de la classe Netoyage   \n",
    "## Execute le netoyage des données     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Netoyage(df)\n",
    "df = N.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des premieres colonnes du dataframe transformé\n",
    "\n",
    "On fait cela pour voir si tout est ok et que le netoyage a bien marché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des colonnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage de la correlation\n",
    "\n",
    "#### 1. Calculer la matrice de corrélation :  \n",
    "J'utilise df.corr() pour calculer la matrice de corrélation pour toutes les colonnes de mon DataFrame df.\n",
    "\n",
    "#### 2. Définir le seuil de corrélation :\n",
    "Je définis un seuil de corrélation de 0.05 pour voir plus tard quelles colonnes ont un impact important.\n",
    "\n",
    "#### 3. Filtrer les colonnes ayant une corrélation faible avec log_price :\n",
    "J'identifie les colonnes dont la corrélation absolue avec log_price est supérieure au seuil défini. J'utilise abs(correlation_matrix[\"log_price\"]) > threshold pour obtenir une liste de ces colonnes.\n",
    "\n",
    "#### 4. Ajouter log_price à la liste des features corrélées :\n",
    "Si log_price n'est pas déjà dans la liste des colonnes corrélées, je l'ajoute. Cela garantit que log_price est toujours inclus dans la matrice de corrélation filtrée.\n",
    "\n",
    "#### 5. Créer une nouvelle matrice de corrélation avec les features filtrées :\n",
    "Je crée une nouvelle matrice de corrélation en utilisant uniquement les colonnes sélectionnées, ce qui permet de se concentrer sur les corrélations pertinentes\n",
    "\n",
    "#### 6. Extraire uniquement la colonne log_price des corrélations filtrées :\n",
    "J'extrais la colonne log_price de cette nouvelle matrice de corrélation pour obtenir uniquement les corrélations des autres variables avec log_price.\n",
    "\n",
    "#### 7. Visualiser les corrélations de log_price avec un heatmap :\n",
    "J'utilise seaborn pour créer une heatmap qui visualise les corrélations entre log_price et les autres variables sélectionnées. J'ajoute des annotations pour afficher les valeurs exactes des corrélations et j'utilise une palette de couleurs coolwarm.\n",
    "\n",
    "#### 8. Afficher les colonnes corrélées dans le DataFrame :\n",
    "J'affiche les colonnes sélectionnées du DataFrame en utilisant df[correlated_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Définir le seuil de corrélation\n",
    "threshold = 0.15\n",
    "\n",
    "# Filtrer les colonnes ayant une corrélation faible avec log_price\n",
    "correlated_features = correlation_matrix.index[abs(correlation_matrix[\"log_price\"]) > threshold].tolist()\n",
    "\n",
    "# Ajouter log_price à la liste des features corrélées si ce n'est pas déjà fait\n",
    "if 'log_price' not in correlated_features:\n",
    "    correlated_features.append('log_price')\n",
    "\n",
    "# Créer une nouvelle matrice de corrélation avec les features filtrées\n",
    "filtered_corr_matrix = df[correlated_features].corr()\n",
    "\n",
    "# Extraire uniquement la colonne 'log_price' des corrélations filtrées\n",
    "log_price_corr = filtered_corr_matrix[['log_price']]\n",
    "\n",
    "# Visualiser les corrélations avec un heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(filtered_corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, cbar=False)\n",
    "plt.title('Correlation with log_price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tableau de correlation\n",
    "\n",
    "1. J'affiche la correlation de chaque colonne avec log_price   \n",
    "2. J'affiche le dataframe avec les colonnes qui ont une grande correlation avec le log_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les corrélations de log_price avec un heatmap\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(filtered_corr_matrix[['log_price']], annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, cbar=False)\n",
    "plt.title('Correlation with log_price')\n",
    "plt.show()\n",
    "\n",
    "df[correlated_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage de la distribution des valeurs des colonnes\n",
    "\n",
    "1. Log Price:\n",
    "Le graphique montre une distribution bimodale des prix (log-transformés), avec des pics principaux autour de 4 et 5. Cela suggère deux groupes principaux de prix de location.\n",
    "\n",
    "2. Room Type:\n",
    "La majorité des Airbnb sont du type \"0\", suivie par \"1\". Le type \"2\" est nettement moins fréquent. (Les types de chambre sont probablement codés numériquement).\n",
    "\n",
    "3. Accommodates:\n",
    "La plupart des propriétés peuvent accueillir entre 1 et 4 personnes, avec une forte concentration sur ceux qui accueillent 2 personnes.\n",
    "\n",
    "4. Bathrooms:\n",
    "La majorité des propriétés ont 1 salle de bain, et le nombre diminue nettement pour 2 salles de bains et plus.\n",
    "\n",
    "5. Bedrooms:\n",
    "La majorité des propriétés a 1 ou 2 chambres à coucher.\n",
    "\n",
    "6. Beds:\n",
    "La distribution montre que la plupart des propriétés ont entre 1 et 3 lits.\n",
    "\n",
    "7. Amenities (TV, Cable TV, Indoor fireplace, etc.):\n",
    "Ces histogrammes montrent la fréquence des différentes commodités. Par exemple, beaucoup de propriétés ont une TV, moins ont le câble, encore moins ont une cheminée intérieure.\n",
    "Les commodités comme les lave-linges, sèche-linges et les éléments adaptés aux familles sont également représentés, chacun avec des fréquences variées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfv=df[correlated_features]\n",
    "numerical_cols = dfv.select_dtypes(include=['int', 'float']).columns\n",
    "fig, axes = plt.subplots(len(numerical_cols), 1, figsize=(4, 4 * len(numerical_cols)))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    dfv[col].hist(ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage d'une carte avec des couleurs des airbnb en fonction du log price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une carte centrée autour des moyennes des latitudes et longitudes\n",
    "import folium\n",
    "\n",
    "data = df\n",
    "map = folium.Map(location=[data['latitude'].mean(), data['longitude'].mean()], zoom_start=10)\n",
    "\n",
    "def color_map(p):\n",
    "    if p < 4:\n",
    "        return \"blue\"\n",
    "    if 4 < p < 5:\n",
    "        return \"orange\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "\n",
    "\n",
    "# Ajouter des points sur la carte pour chaque Airbnb\n",
    "for idx, row in data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        popup=f\"Price: {row['log_price']}\\\\nRoom: {row['room_type']}\\\\nAccommodates: {row['accommodates']}\",\n",
    "        color=color_map(row['log_price']),\n",
    "        fill=True,\n",
    "        fill_color=color_map(row['log_price'])\n",
    "    ).add_to(map)\n",
    "\n",
    "# Afficher la carte\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import des librairies\n",
    "\n",
    "j'importe les librairies que je vais utilisé pour créer mes modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation des Modeles de ML\n",
    "\n",
    "- Séparer les features (X) et la target (y) :  \n",
    "Je commence par séparer les caractéristiques (X) de la variable cible (y) en supprimant la colonne log_price de mon DataFrame pour obtenir X. Ensuite, j'assigne log_price à y.\n",
    "\n",
    "- Diviser les données en ensembles d'entraînement et de test :     \n",
    "Je divise mes données en ensembles d'entraînement et de test en utilisant train_test_split avec 80 % des données pour l'entraînement et 20 % pour le test. J'utilise random_state pour assurer la reproductibilité.\n",
    "\n",
    "- Initialiser les paramètres :     \n",
    "J'initialise les différents paramètres pour initialiser mes modèles\n",
    "\n",
    "- Entrainer le modèle :\n",
    "J'utilise la fonction fit (sauf pour la descente de gradient) pour entrainer mon model sur les 80% de données d'entrainements.\n",
    "\n",
    "- Faire des prédictions :      \n",
    "J'utilise la fonction predict (sauf pour la descente de gradient) pour faire des prédictions sur les ensembles d'entraînement (X_train) et de test (X_test).\n",
    "\n",
    "- Évaluer le modèle :      \n",
    "J'évalue les performances du modèle en calculant le coefficient de détermination (R²) pour les ensembles d'entraînement et de test en utilisant r2_score.\n",
    "\n",
    "- J'affiche les scores R² pour les ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele Descente de Gradient Classique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les features (X) et la target (y)\n",
    "X = df.drop(columns=['log_price'])\n",
    "y = df['log_price']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialiser les paramètres\n",
    "n_samples, n_features = X_train_scaled.shape\n",
    "learning_rate = 0.1\n",
    "n_iterations = 1000\n",
    "\n",
    "# Initialiser les poids et le biais\n",
    "weights = np.zeros(n_features)\n",
    "bias = 0\n",
    "\n",
    "# Descente de gradient batch\n",
    "for i in range(n_iterations):\n",
    "    # Calcul des prédictions\n",
    "    y_pred = np.dot(X_train_scaled, weights) + bias\n",
    "    \n",
    "    # Calcul des gradients\n",
    "    dw = (1 / n_samples) * np.dot(X_train_scaled.T, (y_pred - y_train))\n",
    "    db = (1 / n_samples) * np.sum(y_pred - y_train)\n",
    "    \n",
    "    # Mise à jour des poids et du biais\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred_train = np.dot(X_train_scaled, weights) + bias\n",
    "y_pred_test = np.dot(X_test_scaled, weights) + bias\n",
    "\n",
    "# Évaluer le modèle\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f'R-squared (train): {r2_train}')\n",
    "print(f'R-squared (test): {r2_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele Regression Lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les features (X) et la target (y)\n",
    "X = df.drop(columns=['log_price'])\n",
    "y = df['log_price']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Créer le modèle de régression linéaire\n",
    "model = LinearRegression()\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "# Évaluer le modèle\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f'R-squared (train): {r2_train}')\n",
    "print(f'R-squared (test): {r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les features (X) et la target (y)\n",
    "X = df.drop(columns=['log_price'])\n",
    "y = df['log_price']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer le modèle de régression par forêts aléatoires\n",
    "rfr_model = RandomForestRegressor(n_estimators=50, max_depth=20, random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rfr_model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred_test = rfr_model.predict(X_test)\n",
    "y_pred_train = rfr_model.predict(X_train)\n",
    "\n",
    "# Évaluer le modèle\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f'R-squared (train): {r2_train}')\n",
    "print(f'R-squared (test): {r2_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les features (X) et la target (y)\n",
    "X = df.drop(columns=['log_price'])\n",
    "y = df['log_price']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer le modèle de régression par gradient boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=50, learning_rate=0.5, max_depth=5, random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred_test = gb_model.predict(X_test)\n",
    "y_pred_train = gb_model.predict(X_train)\n",
    "\n",
    "# Évaluer le modèle\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f'R-squared (train): {r2_train}')\n",
    "print(f'R-squared (test): {r2_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les features (X) et la target (y)\n",
    "X = df.drop(columns=['log_price'])\n",
    "y = df['log_price']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "\n",
    "# Creation du model XGRegressor \n",
    "xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, subsample=1, colsample_bytree=1, random_state=20)\n",
    "\n",
    "# Entraîner le modèle\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "y_pred_train= xgb_model.predict(X_train)\n",
    "\n",
    "# Évaluer le modèle\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "print(f'R-squared (train): {r2_train}')\n",
    "print(f'R-squared (test): {r2_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test que j'ai effectué\n",
    "\n",
    "- J'ai testé de standardiser les données pour voir si cela avait une influence sur mon r2 score mais j'ai remarquer que cela n'avait aucune influence pour tout mes modeles.\n",
    "- J'ai aussi essayé d'appliquer le PCA mais cela a diminuer mon r2 score sur mon train et test.\n",
    "- J'ai testé différents parametres pour les modeles pour essayer d'améliorer le r2_score de mes modeles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpretation des résultats\n",
    "\n",
    "On remarque que le meilleur modele que j'ai codé est le XGBoost car j'obtient un r2_score de 0.71 sur le test et 0.90 sur le train.\n",
    "Cela veut dire que 71% des données du test est expliquée par le modele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare les predictions et les vrais prix\n",
    "Sauvegarde les vraies valeurs et les valeurs prédites dans un fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_test = pd.read_csv('airbnb_test.csv')\n",
    "N=Netoyage(airbnb_test)\n",
    "df_test=N.transform()\n",
    "df_train=df.drop('log_price', axis=1)\n",
    "\n",
    "# supprimer les colonnes qui ne sont pas dans df_train\n",
    "for col in df_test.columns:\n",
    "    if col not in df_train.columns:\n",
    "        df_test = df_test.drop(columns=col)\n",
    "\n",
    "# aligner les colonnes de df_train et df_test\n",
    "df_test_aligned = df_test.reindex(columns=df_train.columns)\n",
    "\n",
    "# fait les predictions avec mon meilleur modèle (le XGBoost)\n",
    "pred = xgb_model.predict(df_test_aligned)\n",
    "\n",
    "# sauvegarde les predictions\n",
    "compare = pd.DataFrame({'':airbnb_test['id'],'logpred': pred})\n",
    "compare.to_csv('predictions.csv', index=False)\n",
    "print('Les resultats ont été sauvgarder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de la conformité de mon fichier \"predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estConforme(monFichier_csv):\n",
    "    votre_prediction = pd.read_csv(monFichier_csv)\n",
    "\n",
    "    fichier_exemple = pd.read_csv(\"prediction_example.csv\")\n",
    "\n",
    "    assert votre_prediction.columns[1] == fichier_exemple.columns[1], f\"Attention, votre colonne de prédiction doit s'appeler {fichier_exemple.columns[1]}, elle s'appelle '{votre_prediction.columns[1]}'\"\n",
    "    assert len(votre_prediction) == len(fichier_exemple), f\"Attention, vous devriez avoir {len(fichier_exemple)} prédiction dans votre fichier, il en contient '{len(votre_prediction)}'\"\n",
    "\n",
    "    assert np.all(votre_prediction.iloc[:,0] == fichier_exemple.iloc[:, 0])\n",
    "\n",
    "    print(\"Fichier conforme!\")\n",
    "\n",
    "estConforme(\"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
